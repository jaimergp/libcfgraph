{
 "about": {
  "channels": [
   "https://conda.anaconda.org/conda-forge"
  ],
  "conda_build_version": "3.21.9",
  "conda_private": false,
  "conda_version": "4.13.0",
  "description": "Scrapy is an open source and collaborative framework for extracting the\ndata you need from websites in a fast, simple, yet extensible way.\n",
  "dev_url": "https://github.com/scrapy/scrapy",
  "doc_url": "https://docs.scrapy.org",
  "env_vars": {
   "CIO_TEST": "<not set>"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "rmax",
    "kmike",
    "Gallaecio",
    "wRAR",
    "dangra"
   ]
  },
  "home": "https://scrapy.org/",
  "identifiers": [],
  "keywords": [],
  "license": "BSD-3-Clause-Clear",
  "license_file": "LICENSE",
  "root_pkgs": [
   "pkginfo 1.8.3 pyhd8ed1ab_0",
   "pyyaml 6.0 py39hb9d737c_4",
   "libstdcxx-ng 12.1.0 ha89aaad_16",
   "glob2 0.7 py_0",
   "reproc 14.2.3 h7f98852_0",
   "urllib3 1.26.10 pyhd8ed1ab_0",
   "chardet 5.0.0 py39hf3d152e_0",
   "ruamel_yaml 0.15.80 py39hb9d737c_1007",
   "certifi 2022.6.15 py39hf3d152e_0",
   "libnsl 2.0.0 h7f98852_0",
   "libuuid 2.32.1 h7f98852_1000",
   "ca-certificates 2022.6.15 ha878542_0",
   "reproc-cpp 14.2.3 h9c3ff4c_0",
   "idna 3.3 pyhd8ed1ab_0",
   "patchelf 0.15.0 h58526e2_0",
   "python-fastjsonschema 2.16.1 pyhd8ed1ab_0",
   "zipp 3.8.0 pyhd8ed1ab_0",
   "wheel 0.37.1 pyhd8ed1ab_0",
   "conda-package-handling 1.8.1 py39hb9d737c_1",
   "libxml2 2.9.14 h22db469_3",
   "filelock 3.7.1 pyhd8ed1ab_0",
   "zlib 1.2.12 h166bdaf_2",
   "tini 0.19.0 h7f98852_0",
   "setuptools 63.2.0 py39hf3d152e_0",
   "json5 0.9.5 pyh9f0ad1d_0",
   "anyio 3.6.1 py39hf3d152e_0",
   "pycparser 2.21 pyhd8ed1ab_0",
   "mamba 0.24.0 py39hfa8f2c8_1",
   "nbformat 5.4.0 pyhd8ed1ab_0",
   "python-libarchive-c 4.0 py39hf3d152e_1",
   "jsonschema 4.7.2 pyhd8ed1ab_0",
   "cffi 1.15.1 py39he91dace_0",
   "libmamba 0.24.0 hd8a31e3_1",
   "liblief 0.11.5 h9c3ff4c_1",
   "libnghttp2 1.47.0 h727a467_0",
   "pybind11-abi 4 hd8ed1ab_3",
   "ripgrep 13.0.0 h2f28480_2",
   "c-ares 1.18.1 h7f98852_0",
   "beautifulsoup4 4.11.1 pyha770c72_0",
   "six 1.16.0 pyh6c4a22f_0",
   "clyent 1.2.2 py_1",
   "brotlipy 0.7.0 py39hb9d737c_1004",
   "ld_impl_linux-64 2.36.1 hea4e1c9_2",
   "joblib 1.1.0 pyhd8ed1ab_0",
   "conda 4.13.0 py39hf3d152e_1",
   "tqdm 4.64.0 pyhd8ed1ab_0",
   "zstd 1.5.2 h8a70e8d_2",
   "curl 7.83.1 h7bff187_0",
   "gettext 0.19.8.1 h73d1719_1008",
   "pip 22.2 pyhd8ed1ab_0",
   "python-dateutil 2.8.2 pyhd8ed1ab_0",
   "bzip2 1.0.8 h7f98852_4",
   "libev 4.33 h516909a_1",
   "prompt_toolkit 3.0.30 hd8ed1ab_0",
   "conda-build 3.21.9 py39hf3d152e_1",
   "ncurses 6.3 h27087fc_1",
   "git 2.37.1 pl5321h36853c3_0",
   "importlib_resources 5.8.0 pyhd8ed1ab_0",
   "prompt-toolkit 3.0.30 pyha770c72_0",
   "libgomp 12.1.0 h8d9b700_16",
   "sniffio 1.2.0 py39hf3d152e_3",
   "libffi 3.4.2 h7f98852_5",
   "_libgcc_mutex 0.1 conda_forge",
   "sqlite 3.39.2 h4ff8645_0",
   "requests 2.28.1 pyhd8ed1ab_0",
   "jupyter_core 4.11.1 py39hf3d152e_0",
   "keyutils 1.6.1 h166bdaf_0",
   "anaconda-client 1.8.0 pyhd8ed1ab_0",
   "python 3.9.13 h9a8a25e_0_cpython",
   "yaml-cpp 0.7.0 h27087fc_1",
   "ruamel.yaml 0.17.21 py39hb9d737c_1",
   "cryptography 37.0.4 py39hd97740a_0",
   "pygments 2.12.0 pyhd8ed1ab_0",
   "lz4-c 1.9.3 h9c3ff4c_1",
   "pyopenssl 22.0.0 pyhd8ed1ab_0",
   "psutil 5.9.1 py39hb9d737c_0",
   "perl 5.32.1 2_h7f98852_perl5",
   "watchgod 0.8.2 pyhd8ed1ab_0",
   "boa 0.11.0 pyha770c72_1",
   "markupsafe 2.1.1 py39hb9d737c_1",
   "wcwidth 0.2.5 pyh9f0ad1d_2",
   "readline 8.1.2 h0f457ee_0",
   "pyrsistent 0.18.1 py39hb9d737c_1",
   "ruamel.yaml.clib 0.2.6 py39hb9d737c_1",
   "libsolv 0.7.22 h6239696_0",
   "pysocks 1.7.1 py39hf3d152e_5",
   "libedit 3.1.20191231 he28a2e2_2",
   "python_abi 3.9 2_cp39",
   "libiconv 1.16 h516909a_0",
   "tzdata 2022a h191b570_0",
   "lzo 2.10 h516909a_1000",
   "future 0.18.2 py39hf3d152e_5",
   "su-exec 0.2 h516909a_1002",
   "openssl 1.1.1q h166bdaf_0",
   "dataclasses 0.8 pyhc8e2a94_3",
   "libgcc-ng 12.1.0 h8d9b700_16",
   "charset-normalizer 2.1.0 pyhd8ed1ab_0",
   "libcurl 7.83.1 h7bff187_0",
   "jinja2 3.1.2 pyhd8ed1ab_1",
   "rich 12.5.1 pyhd8ed1ab_0",
   "traitlets 5.3.0 pyhd8ed1ab_0",
   "typing_extensions 4.3.0 pyha770c72_0",
   "xz 5.2.5 h516909a_1",
   "libarchive 3.5.2 hb890918_3",
   "krb5 1.19.3 h3790be6_0",
   "importlib-metadata 4.11.4 py39hf3d152e_0",
   "pycosat 0.6.3 py39hb9d737c_1010",
   "pytz 2022.1 pyhd8ed1ab_0",
   "commonmark 0.9.1 py_0",
   "pcre2 10.37 h032f7d1_0",
   "colorama 0.4.5 pyhd8ed1ab_0",
   "backports.functools_lru_cache 1.6.4 pyhd8ed1ab_0",
   "attrs 21.4.0 pyhd8ed1ab_0",
   "_openmp_mutex 4.5 2_gnu",
   "patch 2.7.6 h7f98852_1002",
   "expat 2.4.8 h27087fc_0",
   "libssh2 1.10.0 ha56f1ee_2",
   "py-lief 0.11.5 py39he80948d_1",
   "libzlib 1.2.12 h166bdaf_2",
   "yaml 0.2.5 h7f98852_2",
   "libmambapy 0.24.0 py39hd55135b_1",
   "backports 1.0 py_2",
   "tk 8.6.12 h27826a3_0",
   "soupsieve 2.3.2.post1 pyhd8ed1ab_0",
   "icu 70.1 h27087fc_0",
   "conda-forge-ci-setup 3.21.0 py39h69ce9fc_100",
   "jq 1.6 h36c2ea0_1000",
   "shyaml 0.6.2 pyhd3deb0d_0",
   "conda-env 2.6.0 1",
   "click 8.1.3 py39hf3d152e_0",
   "oniguruma 6.9.8 h166bdaf_0"
  ],
  "summary": "A high-level Python Screen Scraping framework",
  "tags": []
 },
 "conda_build_config": {
  "CI": "azure",
  "c_compiler": "gcc",
  "cdt_name": "cos6",
  "channel_sources": "conda-forge",
  "channel_targets": "conda-forge main",
  "cpu_optimization_target": "nocona",
  "cran_mirror": "https://cran.r-project.org",
  "cxx_compiler": "gxx",
  "docker_image": "quay.io/condaforge/linux-anvil-cos7-x86_64",
  "extend_keys": [
   "pin_run_as_build",
   "ignore_build_only_deps",
   "ignore_version",
   "extend_keys"
  ],
  "fortran_compiler": "gfortran",
  "ignore_build_only_deps": [
   "python",
   "numpy"
  ],
  "lua": "5",
  "numpy": "1.16",
  "perl": "5.26.2",
  "pin_run_as_build": {
   "python": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   },
   "r-base": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   }
  },
  "python": "3.10.* *_cpython",
  "r_base": "3.5",
  "target_platform": "linux-64"
 },
 "files": [
  "bin/scrapy",
  "lib/python3.10/site-packages/Scrapy-2.6.2.dist-info/AUTHORS",
  "lib/python3.10/site-packages/Scrapy-2.6.2.dist-info/INSTALLER",
  "lib/python3.10/site-packages/Scrapy-2.6.2.dist-info/LICENSE",
  "lib/python3.10/site-packages/Scrapy-2.6.2.dist-info/METADATA",
  "lib/python3.10/site-packages/Scrapy-2.6.2.dist-info/RECORD",
  "lib/python3.10/site-packages/Scrapy-2.6.2.dist-info/REQUESTED",
  "lib/python3.10/site-packages/Scrapy-2.6.2.dist-info/WHEEL",
  "lib/python3.10/site-packages/Scrapy-2.6.2.dist-info/direct_url.json",
  "lib/python3.10/site-packages/scrapy/VERSION",
  "lib/python3.10/site-packages/scrapy/__init__.py",
  "lib/python3.10/site-packages/scrapy/__main__.py",
  "lib/python3.10/site-packages/scrapy/cmdline.py",
  "lib/python3.10/site-packages/scrapy/commands/__init__.py",
  "lib/python3.10/site-packages/scrapy/commands/bench.py",
  "lib/python3.10/site-packages/scrapy/commands/check.py",
  "lib/python3.10/site-packages/scrapy/commands/crawl.py",
  "lib/python3.10/site-packages/scrapy/commands/edit.py",
  "lib/python3.10/site-packages/scrapy/commands/fetch.py",
  "lib/python3.10/site-packages/scrapy/commands/genspider.py",
  "lib/python3.10/site-packages/scrapy/commands/list.py",
  "lib/python3.10/site-packages/scrapy/commands/parse.py",
  "lib/python3.10/site-packages/scrapy/commands/runspider.py",
  "lib/python3.10/site-packages/scrapy/commands/settings.py",
  "lib/python3.10/site-packages/scrapy/commands/shell.py",
  "lib/python3.10/site-packages/scrapy/commands/startproject.py",
  "lib/python3.10/site-packages/scrapy/commands/version.py",
  "lib/python3.10/site-packages/scrapy/commands/view.py",
  "lib/python3.10/site-packages/scrapy/contracts/__init__.py",
  "lib/python3.10/site-packages/scrapy/contracts/default.py",
  "lib/python3.10/site-packages/scrapy/core/__init__.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/__init__.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/contextfactory.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/handlers/__init__.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/handlers/datauri.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/handlers/file.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/handlers/ftp.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/handlers/http.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/handlers/http10.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/handlers/http11.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/handlers/http2.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/handlers/s3.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/middleware.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/tls.py",
  "lib/python3.10/site-packages/scrapy/core/downloader/webclient.py",
  "lib/python3.10/site-packages/scrapy/core/engine.py",
  "lib/python3.10/site-packages/scrapy/core/http2/__init__.py",
  "lib/python3.10/site-packages/scrapy/core/http2/agent.py",
  "lib/python3.10/site-packages/scrapy/core/http2/protocol.py",
  "lib/python3.10/site-packages/scrapy/core/http2/stream.py",
  "lib/python3.10/site-packages/scrapy/core/scheduler.py",
  "lib/python3.10/site-packages/scrapy/core/scraper.py",
  "lib/python3.10/site-packages/scrapy/core/spidermw.py",
  "lib/python3.10/site-packages/scrapy/crawler.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/__init__.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/ajaxcrawl.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/cookies.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/decompression.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/defaultheaders.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/downloadtimeout.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/httpauth.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/httpcache.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/httpcompression.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/httpproxy.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/redirect.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/retry.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/robotstxt.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/stats.py",
  "lib/python3.10/site-packages/scrapy/downloadermiddlewares/useragent.py",
  "lib/python3.10/site-packages/scrapy/dupefilters.py",
  "lib/python3.10/site-packages/scrapy/exceptions.py",
  "lib/python3.10/site-packages/scrapy/exporters.py",
  "lib/python3.10/site-packages/scrapy/extension.py",
  "lib/python3.10/site-packages/scrapy/extensions/__init__.py",
  "lib/python3.10/site-packages/scrapy/extensions/closespider.py",
  "lib/python3.10/site-packages/scrapy/extensions/corestats.py",
  "lib/python3.10/site-packages/scrapy/extensions/debug.py",
  "lib/python3.10/site-packages/scrapy/extensions/feedexport.py",
  "lib/python3.10/site-packages/scrapy/extensions/httpcache.py",
  "lib/python3.10/site-packages/scrapy/extensions/logstats.py",
  "lib/python3.10/site-packages/scrapy/extensions/memdebug.py",
  "lib/python3.10/site-packages/scrapy/extensions/memusage.py",
  "lib/python3.10/site-packages/scrapy/extensions/postprocessing.py",
  "lib/python3.10/site-packages/scrapy/extensions/spiderstate.py",
  "lib/python3.10/site-packages/scrapy/extensions/statsmailer.py",
  "lib/python3.10/site-packages/scrapy/extensions/telnet.py",
  "lib/python3.10/site-packages/scrapy/extensions/throttle.py",
  "lib/python3.10/site-packages/scrapy/http/__init__.py",
  "lib/python3.10/site-packages/scrapy/http/common.py",
  "lib/python3.10/site-packages/scrapy/http/cookies.py",
  "lib/python3.10/site-packages/scrapy/http/headers.py",
  "lib/python3.10/site-packages/scrapy/http/request/__init__.py",
  "lib/python3.10/site-packages/scrapy/http/request/form.py",
  "lib/python3.10/site-packages/scrapy/http/request/json_request.py",
  "lib/python3.10/site-packages/scrapy/http/request/rpc.py",
  "lib/python3.10/site-packages/scrapy/http/response/__init__.py",
  "lib/python3.10/site-packages/scrapy/http/response/html.py",
  "lib/python3.10/site-packages/scrapy/http/response/text.py",
  "lib/python3.10/site-packages/scrapy/http/response/xml.py",
  "lib/python3.10/site-packages/scrapy/interfaces.py",
  "lib/python3.10/site-packages/scrapy/item.py",
  "lib/python3.10/site-packages/scrapy/link.py",
  "lib/python3.10/site-packages/scrapy/linkextractors/__init__.py",
  "lib/python3.10/site-packages/scrapy/linkextractors/lxmlhtml.py",
  "lib/python3.10/site-packages/scrapy/loader/__init__.py",
  "lib/python3.10/site-packages/scrapy/loader/common.py",
  "lib/python3.10/site-packages/scrapy/loader/processors.py",
  "lib/python3.10/site-packages/scrapy/logformatter.py",
  "lib/python3.10/site-packages/scrapy/mail.py",
  "lib/python3.10/site-packages/scrapy/middleware.py",
  "lib/python3.10/site-packages/scrapy/mime.types",
  "lib/python3.10/site-packages/scrapy/pipelines/__init__.py",
  "lib/python3.10/site-packages/scrapy/pipelines/files.py",
  "lib/python3.10/site-packages/scrapy/pipelines/images.py",
  "lib/python3.10/site-packages/scrapy/pipelines/media.py",
  "lib/python3.10/site-packages/scrapy/pqueues.py",
  "lib/python3.10/site-packages/scrapy/resolver.py",
  "lib/python3.10/site-packages/scrapy/responsetypes.py",
  "lib/python3.10/site-packages/scrapy/robotstxt.py",
  "lib/python3.10/site-packages/scrapy/selector/__init__.py",
  "lib/python3.10/site-packages/scrapy/selector/unified.py",
  "lib/python3.10/site-packages/scrapy/settings/__init__.py",
  "lib/python3.10/site-packages/scrapy/settings/default_settings.py",
  "lib/python3.10/site-packages/scrapy/shell.py",
  "lib/python3.10/site-packages/scrapy/signalmanager.py",
  "lib/python3.10/site-packages/scrapy/signals.py",
  "lib/python3.10/site-packages/scrapy/spiderloader.py",
  "lib/python3.10/site-packages/scrapy/spidermiddlewares/__init__.py",
  "lib/python3.10/site-packages/scrapy/spidermiddlewares/depth.py",
  "lib/python3.10/site-packages/scrapy/spidermiddlewares/httperror.py",
  "lib/python3.10/site-packages/scrapy/spidermiddlewares/offsite.py",
  "lib/python3.10/site-packages/scrapy/spidermiddlewares/referer.py",
  "lib/python3.10/site-packages/scrapy/spidermiddlewares/urllength.py",
  "lib/python3.10/site-packages/scrapy/spiders/__init__.py",
  "lib/python3.10/site-packages/scrapy/spiders/crawl.py",
  "lib/python3.10/site-packages/scrapy/spiders/feed.py",
  "lib/python3.10/site-packages/scrapy/spiders/init.py",
  "lib/python3.10/site-packages/scrapy/spiders/sitemap.py",
  "lib/python3.10/site-packages/scrapy/squeues.py",
  "lib/python3.10/site-packages/scrapy/statscollectors.py",
  "lib/python3.10/site-packages/scrapy/templates/project/module/__init__.py",
  "lib/python3.10/site-packages/scrapy/templates/project/module/items.py.tmpl",
  "lib/python3.10/site-packages/scrapy/templates/project/module/middlewares.py.tmpl",
  "lib/python3.10/site-packages/scrapy/templates/project/module/pipelines.py.tmpl",
  "lib/python3.10/site-packages/scrapy/templates/project/module/settings.py.tmpl",
  "lib/python3.10/site-packages/scrapy/templates/project/module/spiders/__init__.py",
  "lib/python3.10/site-packages/scrapy/templates/project/scrapy.cfg",
  "lib/python3.10/site-packages/scrapy/templates/spiders/basic.tmpl",
  "lib/python3.10/site-packages/scrapy/templates/spiders/crawl.tmpl",
  "lib/python3.10/site-packages/scrapy/templates/spiders/csvfeed.tmpl",
  "lib/python3.10/site-packages/scrapy/templates/spiders/xmlfeed.tmpl",
  "lib/python3.10/site-packages/scrapy/utils/__init__.py",
  "lib/python3.10/site-packages/scrapy/utils/asyncgen.py",
  "lib/python3.10/site-packages/scrapy/utils/benchserver.py",
  "lib/python3.10/site-packages/scrapy/utils/boto.py",
  "lib/python3.10/site-packages/scrapy/utils/conf.py",
  "lib/python3.10/site-packages/scrapy/utils/console.py",
  "lib/python3.10/site-packages/scrapy/utils/curl.py",
  "lib/python3.10/site-packages/scrapy/utils/datatypes.py",
  "lib/python3.10/site-packages/scrapy/utils/decorators.py",
  "lib/python3.10/site-packages/scrapy/utils/defer.py",
  "lib/python3.10/site-packages/scrapy/utils/deprecate.py",
  "lib/python3.10/site-packages/scrapy/utils/display.py",
  "lib/python3.10/site-packages/scrapy/utils/engine.py",
  "lib/python3.10/site-packages/scrapy/utils/ftp.py",
  "lib/python3.10/site-packages/scrapy/utils/gz.py",
  "lib/python3.10/site-packages/scrapy/utils/httpobj.py",
  "lib/python3.10/site-packages/scrapy/utils/iterators.py",
  "lib/python3.10/site-packages/scrapy/utils/job.py",
  "lib/python3.10/site-packages/scrapy/utils/log.py",
  "lib/python3.10/site-packages/scrapy/utils/misc.py",
  "lib/python3.10/site-packages/scrapy/utils/ossignal.py",
  "lib/python3.10/site-packages/scrapy/utils/project.py",
  "lib/python3.10/site-packages/scrapy/utils/py36.py",
  "lib/python3.10/site-packages/scrapy/utils/python.py",
  "lib/python3.10/site-packages/scrapy/utils/reactor.py",
  "lib/python3.10/site-packages/scrapy/utils/reqser.py",
  "lib/python3.10/site-packages/scrapy/utils/request.py",
  "lib/python3.10/site-packages/scrapy/utils/response.py",
  "lib/python3.10/site-packages/scrapy/utils/serialize.py",
  "lib/python3.10/site-packages/scrapy/utils/signal.py",
  "lib/python3.10/site-packages/scrapy/utils/sitemap.py",
  "lib/python3.10/site-packages/scrapy/utils/spider.py",
  "lib/python3.10/site-packages/scrapy/utils/ssl.py",
  "lib/python3.10/site-packages/scrapy/utils/template.py",
  "lib/python3.10/site-packages/scrapy/utils/test.py",
  "lib/python3.10/site-packages/scrapy/utils/testproc.py",
  "lib/python3.10/site-packages/scrapy/utils/testsite.py",
  "lib/python3.10/site-packages/scrapy/utils/trackref.py",
  "lib/python3.10/site-packages/scrapy/utils/url.py",
  "lib/python3.10/site-packages/scrapy/utils/versions.py"
 ],
 "index": {
  "arch": "x86_64",
  "build": "py310hff52083_0",
  "build_number": 0,
  "depends": [
   "cryptography >=2.0",
   "cssselect >=0.9.1",
   "itemadapter >=0.1.0",
   "itemloaders >=1.0.1",
   "libxml2 <2.9.11",
   "lxml >=3.5.0",
   "parsel >=1.5.0",
   "protego >=0.1.15",
   "pydispatcher >=2.0.5",
   "pyopenssl >=16.2.0",
   "python >=3.10,<3.11.0a0",
   "python_abi 3.10.* *_cp310",
   "queuelib >=1.4.2",
   "service_identity >=16.0.0",
   "setuptools",
   "tldextract",
   "twisted >=17.9.0",
   "w3lib >=1.17.0",
   "zope.interface >=4.1.3"
  ],
  "license": "BSD-3-Clause-Clear",
  "name": "scrapy",
  "platform": "linux",
  "subdir": "linux-64",
  "timestamp": 1658818476203,
  "version": "2.6.2"
 },
 "metadata_version": 1,
 "name": "scrapy",
 "raw_recipe": "#\n# See the guidelines for updating this recipe for new releases:\n# https://github.com/scrapy/scrapy/wiki/Scrapy-release-procedure#conda-packages\n#\n{% set name = \"Scrapy\" %}\n{% set version = \"2.6.2\" %}\n{% set hash_val = \"55e21181165f25337105fff1efc8393296375cea7de699a7e703bbd265595f26\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  fn: {{ name }}-{{ version }}.tar.gz\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: {{ hash_val }}\n\nbuild:\n  script: {{ PYTHON }} -m pip install . --no-deps -vv\n  number: 0\n  skip: true  # [py2k]\n  entry_points:\n    - scrapy = scrapy.cmdline:execute\n\n# The requirements below shall match the requirements from the target package\n# version (check setup.py for changes).\nrequirements:\n  host:\n    - python\n    - pip\n    - setuptools\n\n  run:\n    - python\n    - pywin32  # [win]\n\n    # Same order as setup.py:\n    - twisted >=17.9.0\n    - cryptography >=2.0\n    - cssselect >=0.9.1\n    - itemloaders >=1.0.1\n    - lxml >=3.5.0\n    - parsel >=1.5.0\n    - pydispatcher >=2.0.5\n    - pyOpenSSL >=16.2.0\n    - queuelib >=1.4.2\n    - service_identity >=16.0.0\n    - w3lib >=1.17.0\n    - zope.interface >=4.1.3\n    - protego >=0.1.15\n    - itemadapter >=0.1.0\n    - setuptools\n    - tldextract\n    # https://github.com/scrapy/scrapy/pull/5208\n    - libxml2 <2.9.11\n\ntest:\n  requires:\n    - attrs\n    - botocore\n    - brotlipy\n    - bpython  # [not win]\n    - dataclasses  # [py!=35]\n    - ipython\n    - google-cloud-storage\n    - pip\n    - jmespath\n    - pillow\n    - pytest\n    - testfixtures\n    - priority >=1.1.0, <2.0\n  source_files:\n    - conftest.py\n    - scrapy\n    - tests\n  commands:\n    - scrapy version -v\n    - pip install setuptools-scm  # required by indirect dependency pytest-forked\n    # test requirements unavailable on conda:\n    - pip install sybil\n\n    # tests.test_proxy_connect on python 3.7 needs old mitmproxy and markupsafe.\n    - pip install 'mitmproxy<8'  # [py>=37 and py<39]\n    - pip install 'mitmproxy<5'  # [py<37]\n    - pip install 'markupsafe<2.1.0'  # [py<=37]\n\n    - mv scrapy _scrapy\n    # https://github.com/scrapy/scrapy/issues/2653#issuecomment-598610517\n    - rm tests/test_utils_asyncio.py  # [win and py==38]\n    - rm -f tests/test_command_parse.py tests/test_commands.py tests/test_crawler.py tests/test_downloader_handlers.py tests/test_downloader_handlers_http2.py  # [osx]\n    # Test failures since Scrapy 2.4:\n    - rm -f tests/test_command_check.py tests/test_command_parse.py tests/test_commands.py tests/test_crawler.py tests/test_feedexport.py\n    # test_convert_image fails because expected vs actual color does not match\n    # by 1 bit (#0079FF vs #0080FF)\n    - rm -f tests/test_pipeline_images.py\n    # Tests need to be fixed in upstream.\n    - pytest _scrapy tests\n\nabout:\n  home: https://scrapy.org/\n  license: BSD-3-Clause-Clear\n  license_file: LICENSE\n  summary: A high-level Python Screen Scraping framework\n  description: |\n    Scrapy is an open source and collaborative framework for extracting the\n    data you need from websites in a fast, simple, yet extensible way.\n  doc_url: https://docs.scrapy.org\n  dev_url: https://github.com/scrapy/scrapy\n\nextra:\n  recipe-maintainers:\n    - rmax\n    - kmike\n    - Gallaecio\n    - wRAR\n    - dangra\n",
 "rendered_recipe": {
  "about": {
   "description": "Scrapy is an open source and collaborative framework for extracting the\ndata you need from websites in a fast, simple, yet extensible way.\n",
   "dev_url": "https://github.com/scrapy/scrapy",
   "doc_url": "https://docs.scrapy.org",
   "home": "https://scrapy.org/",
   "license": "BSD-3-Clause-Clear",
   "license_file": "LICENSE",
   "summary": "A high-level Python Screen Scraping framework"
  },
  "build": {
   "entry_points": [
    "scrapy = scrapy.cmdline:execute"
   ],
   "number": "0",
   "script": "/home/conda/feedstock_root/build_artifacts/scrapy_1658818376603/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin/python -m pip install . --no-deps -vv",
   "string": "py310hff52083_0"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "Gallaecio",
    "dangra",
    "kmike",
    "rmax",
    "wRAR"
   ]
  },
  "package": {
   "name": "scrapy",
   "version": "2.6.2"
  },
  "requirements": {
   "host": [
    "_libgcc_mutex 0.1 conda_forge",
    "_openmp_mutex 4.5 2_gnu",
    "bzip2 1.0.8 h7f98852_4",
    "ca-certificates 2022.6.15 ha878542_0",
    "ld_impl_linux-64 2.36.1 hea4e1c9_2",
    "libffi 3.4.2 h7f98852_5",
    "libgcc-ng 12.1.0 h8d9b700_16",
    "libgomp 12.1.0 h8d9b700_16",
    "libnsl 2.0.0 h7f98852_0",
    "libuuid 2.32.1 h7f98852_1000",
    "libzlib 1.2.12 h166bdaf_2",
    "ncurses 6.3 h27087fc_1",
    "openssl 3.0.5 h166bdaf_0",
    "pip 22.2 pyhd8ed1ab_0",
    "python 3.10.5 ha86cf86_0_cpython",
    "python_abi 3.10 2_cp310",
    "readline 8.1.2 h0f457ee_0",
    "setuptools 63.2.0 py310hff52083_0",
    "sqlite 3.39.2 h4ff8645_0",
    "tk 8.6.12 h27826a3_0",
    "tzdata 2022a h191b570_0",
    "wheel 0.37.1 pyhd8ed1ab_0",
    "xz 5.2.5 h516909a_1",
    "zlib 1.2.12 h166bdaf_2"
   ],
   "run": [
    "cryptography >=2.0",
    "cssselect >=0.9.1",
    "itemadapter >=0.1.0",
    "itemloaders >=1.0.1",
    "libxml2 <2.9.11",
    "lxml >=3.5.0",
    "parsel >=1.5.0",
    "protego >=0.1.15",
    "pyOpenSSL >=16.2.0",
    "pydispatcher >=2.0.5",
    "python >=3.10,<3.11.0a0",
    "python_abi 3.10.* *_cp310",
    "queuelib >=1.4.2",
    "service_identity >=16.0.0",
    "setuptools",
    "tldextract",
    "twisted >=17.9.0",
    "w3lib >=1.17.0",
    "zope.interface >=4.1.3"
   ]
  },
  "source": {
   "fn": "Scrapy-2.6.2.tar.gz",
   "sha256": "55e21181165f25337105fff1efc8393296375cea7de699a7e703bbd265595f26",
   "url": "https://pypi.io/packages/source/S/Scrapy/Scrapy-2.6.2.tar.gz"
  },
  "test": {
   "commands": [
    "scrapy version -v",
    "pip install setuptools-scm",
    "pip install sybil",
    "mv scrapy _scrapy",
    "rm -f tests/test_command_check.py tests/test_command_parse.py tests/test_commands.py tests/test_crawler.py tests/test_feedexport.py",
    "rm -f tests/test_pipeline_images.py",
    "pytest _scrapy tests"
   ],
   "requires": [
    "attrs",
    "botocore",
    "bpython",
    "brotlipy",
    "dataclasses",
    "google-cloud-storage",
    "ipython",
    "jmespath",
    "pillow",
    "pip",
    "priority >=1.1.0, <2.0",
    "pytest",
    "testfixtures"
   ],
   "source_files": [
    "conftest.py",
    "scrapy",
    "tests"
   ]
  }
 },
 "version": "2.6.2"
}