{
 "about": {
  "channels": [
   "https://conda.anaconda.org/conda-forge"
  ],
  "conda_build_version": "3.21.8",
  "conda_private": false,
  "conda_version": "4.12.0",
  "description": "The FinetuningScheduler callback accelerates and enhances foundational model experimentation with flexible finetuning\nschedules. Training with the FinetuningScheduler callback is simple and confers a host of benefits:\n\n- it dramatically increases finetuning flexibility\n- expedites and facilitates exploration of model tuning dynamics\n- enables marginal performance improvements of finetuned models\n\nFundamentally, the FinetuningScheduler callback enables multi-phase, scheduled finetuning of foundational models.\nGradual unfreezing (i.e. thawing) can help maximize foundational model knowledge retention while allowing (typically\nupper layers of) the model to optimally adapt to new tasks during transfer learning.\n\nFinetuningScheduler orchestrates the gradual unfreezing of models via a finetuning schedule that is either implicitly\ngenerated (the default) or explicitly provided by the user (more computationally efficient). Finetuning phase\ntransitions are driven by FTSEarlyStopping criteria (a multi-phase extension of EarlyStopping), user-specified epoch\ntransitions or a composition of the two (the default mode). A FinetuningScheduler training session completes when the\nfinal phase of the schedule has its stopping criteria met.\n\nDocumentation\n-------------\n- https://finetuning-scheduler.readthedocs.io/en/stable/\n- https://finetuning-scheduler.readthedocs.io/en/0.1.3/\n",
  "env_vars": {
   "CIO_TEST": "<not set>"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "speediedan"
   ]
  },
  "home": "https://github.com/speediedan/finetuning-scheduler",
  "identifiers": [],
  "keywords": [],
  "license": "Apache-2.0",
  "license_file": "LICENSE",
  "root_pkgs": [
   "importlib_resources 5.7.1 pyhd8ed1ab_0",
   "traitlets 5.1.1 pyhd8ed1ab_0",
   "libcurl 7.83.0 h7bff187_0",
   "expat 2.4.8 h27087fc_0",
   "urllib3 1.26.9 pyhd8ed1ab_0",
   "charset-normalizer 2.0.12 pyhd8ed1ab_0",
   "json5 0.9.5 pyh9f0ad1d_0",
   "libstdcxx-ng 11.2.0 he4da1e4_16",
   "pytz 2022.1 pyhd8ed1ab_0",
   "curl 7.83.0 h7bff187_0",
   "libnsl 2.0.0 h7f98852_0",
   "reproc 14.2.3 h7f98852_0",
   "c-ares 1.18.1 h7f98852_0",
   "ca-certificates 2021.10.8 ha878542_0",
   "pycosat 0.6.3 py39hb9d737c_1010",
   "chardet 4.0.0 py39hf3d152e_3",
   "ruamel_yaml 0.15.80 py39h3811e60_1006",
   "wheel 0.37.1 pyhd8ed1ab_0",
   "colorama 0.4.4 pyh9f0ad1d_0",
   "tini 0.19.0 h7f98852_0",
   "requests 2.27.1 pyhd8ed1ab_0",
   "libarchive 3.5.2 hccf745f_1",
   "patchelf 0.14.5 h58526e2_0",
   "_openmp_mutex 4.5 2_gnu",
   "libev 4.33 h516909a_1",
   "jupyter_core 4.9.2 py39hf3d152e_0",
   "libsolv 0.7.22 h6239696_0",
   "tqdm 4.64.0 pyhd8ed1ab_0",
   "zstd 1.5.2 ha95c52a_0",
   "ruamel.yaml 0.17.21 py39hb9d737c_1",
   "certifi 2021.10.8 py39hf3d152e_2",
   "pyyaml 6.0 py39hb9d737c_4",
   "libiconv 1.16 h516909a_0",
   "anyio 3.5.0 py39hf3d152e_0",
   "conda 4.12.0 py39hf3d152e_0",
   "jsonschema 4.4.0 pyhd8ed1ab_0",
   "lz4-c 1.9.3 h9c3ff4c_1",
   "su-exec 0.2 h516909a_1002",
   "brotlipy 0.7.0 py39hb9d737c_1004",
   "zlib 1.2.11 h166bdaf_1014",
   "filelock 3.6.0 pyhd8ed1ab_0",
   "yaml 0.2.5 h7f98852_2",
   "joblib 1.1.0 pyhd8ed1ab_0",
   "backports 1.0 py_2",
   "krb5 1.19.3 h3790be6_0",
   "sqlite 3.38.3 h4ff8645_0",
   "python_abi 3.9 2_cp39",
   "pycparser 2.21 pyhd8ed1ab_0",
   "clyent 1.2.2 py_1",
   "attrs 21.4.0 pyhd8ed1ab_0",
   "libuuid 2.32.1 h7f98852_1000",
   "cffi 1.15.0 py39h4bc2ebd_0",
   "pcre2 10.37 h032f7d1_0",
   "sniffio 1.2.0 py39hf3d152e_3",
   "nbformat 5.3.0 pyhd8ed1ab_0",
   "python-fastjsonschema 2.15.3 pyhd8ed1ab_0",
   "readline 8.1 h46c0cb4_0",
   "libmamba 0.23.0 hd8a31e3_1",
   "psutil 5.9.0 py39hb9d737c_1",
   "ld_impl_linux-64 2.36.1 hea4e1c9_2",
   "libffi 3.4.2 h7f98852_5",
   "watchgod 0.8.2 pyhd8ed1ab_0",
   "git 2.35.3 pl5321h36853c3_0",
   "icu 70.1 h27087fc_0",
   "bzip2 1.0.8 h7f98852_4",
   "pysocks 1.7.1 py39hf3d152e_5",
   "future 0.18.2 py39hf3d152e_5",
   "conda-package-handling 1.8.1 py39hb9d737c_1",
   "pyrsistent 0.18.1 py39hb9d737c_1",
   "python-libarchive-c 4.0 py39hf3d152e_1",
   "idna 3.3 pyhd8ed1ab_0",
   "xz 5.2.5 h516909a_1",
   "libgcc-ng 11.2.0 h1d223b6_16",
   "setuptools 62.1.0 py39hf3d152e_0",
   "markupsafe 2.1.1 py39hb9d737c_1",
   "libedit 3.1.20191231 he28a2e2_2",
   "anaconda-client 1.8.0 pyhd8ed1ab_0",
   "ripgrep 13.0.0 h2f28480_2",
   "commonmark 0.9.1 py_0",
   "python 3.9.12 h9a8a25e_1_cpython",
   "prompt-toolkit 3.0.29 pyha770c72_0",
   "beautifulsoup4 4.11.1 pyha770c72_0",
   "liblief 0.11.5 h9c3ff4c_1",
   "wcwidth 0.2.5 pyh9f0ad1d_2",
   "libzlib 1.2.11 h166bdaf_1014",
   "tzdata 2022a h191b570_0",
   "yaml-cpp 0.6.3 he1b5a44_4",
   "glob2 0.7 py_0",
   "py-lief 0.11.5 py39he80948d_1",
   "libxml2 2.9.13 h22db469_0",
   "tk 8.6.12 h27826a3_0",
   "pyopenssl 22.0.0 pyhd8ed1ab_0",
   "reproc-cpp 14.2.3 h9c3ff4c_0",
   "patch 2.7.6 h7f98852_1002",
   "six 1.16.0 pyh6c4a22f_0",
   "mamba 0.23.0 py39hfa8f2c8_1",
   "python-dateutil 2.8.2 pyhd8ed1ab_0",
   "zipp 3.8.0 pyhd8ed1ab_0",
   "pkginfo 1.8.2 pyhd8ed1ab_0",
   "dataclasses 0.8 pyhc8e2a94_3",
   "ruamel.yaml.clib 0.2.6 py39hb9d737c_1",
   "libgomp 11.2.0 h1d223b6_16",
   "libssh2 1.10.0 ha56f1ee_2",
   "lzo 2.10 h516909a_1000",
   "jinja2 3.1.1 pyhd8ed1ab_0",
   "pygments 2.12.0 pyhd8ed1ab_0",
   "keyutils 1.6.1 h166bdaf_0",
   "cryptography 36.0.2 py39hd97740a_1",
   "ncurses 6.3 h27087fc_1",
   "pip 22.0.4 pyhd8ed1ab_0",
   "perl 5.32.1 2_h7f98852_perl5",
   "libmambapy 0.23.0 py39hd55135b_1",
   "libnghttp2 1.47.0 h727a467_0",
   "pybind11-abi 4 hd8ed1ab_3",
   "boa 0.11.0 pyha770c72_0",
   "backports.functools_lru_cache 1.6.4 pyhd8ed1ab_0",
   "prompt_toolkit 3.0.29 hd8ed1ab_0",
   "typing_extensions 4.2.0 pyha770c72_1",
   "importlib-metadata 4.11.3 py39hf3d152e_1",
   "conda-build 3.21.8 py39hf3d152e_0",
   "rich 12.3.0 pyhd8ed1ab_0",
   "soupsieve 2.3.1 pyhd8ed1ab_0",
   "_libgcc_mutex 0.1 conda_forge",
   "gettext 0.19.8.1 h73d1719_1008",
   "shyaml 0.6.2 pyhd3deb0d_0",
   "conda-env 2.6.0 1",
   "click 8.1.3 py39hf3d152e_0",
   "conda-forge-ci-setup 3.20.0 py39h69ce9fc_100",
   "oniguruma 6.9.7.1 h7f98852_0",
   "jq 1.6 h36c2ea0_1000",
   "openssl 1.1.1o h166bdaf_0"
  ],
  "summary": "A PyTorch Lightning extension that enhances model experimentation with flexible finetuning schedules.",
  "tags": []
 },
 "conda_build_config": {
  "CI": "azure",
  "c_compiler": "gcc",
  "cdt_name": "cos6",
  "channel_sources": "conda-forge",
  "channel_targets": "conda-forge main",
  "cpu_optimization_target": "nocona",
  "cran_mirror": "https://cran.r-project.org",
  "cxx_compiler": "gxx",
  "docker_image": "quay.io/condaforge/linux-anvil-cos7-x86_64",
  "extend_keys": [
   "ignore_build_only_deps",
   "extend_keys",
   "pin_run_as_build",
   "ignore_version"
  ],
  "fortran_compiler": "gfortran",
  "ignore_build_only_deps": [
   "python",
   "numpy"
  ],
  "lua": "5",
  "numpy": "1.16",
  "perl": "5.26.2",
  "pin_run_as_build": {
   "python": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   },
   "r-base": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   }
  },
  "python": "3.9",
  "r_base": "3.5",
  "target_platform": "linux-64"
 },
 "files": [
  "site-packages/finetuning_scheduler-0.1.3.dist-info/INSTALLER",
  "site-packages/finetuning_scheduler-0.1.3.dist-info/LICENSE",
  "site-packages/finetuning_scheduler-0.1.3.dist-info/METADATA",
  "site-packages/finetuning_scheduler-0.1.3.dist-info/RECORD",
  "site-packages/finetuning_scheduler-0.1.3.dist-info/REQUESTED",
  "site-packages/finetuning_scheduler-0.1.3.dist-info/WHEEL",
  "site-packages/finetuning_scheduler-0.1.3.dist-info/direct_url.json",
  "site-packages/finetuning_scheduler/__about__.py",
  "site-packages/finetuning_scheduler/__init__.py",
  "site-packages/finetuning_scheduler/fts.py",
  "site-packages/finetuning_scheduler/fts_supporters.py",
  "site-packages/finetuning_scheduler/py.typed",
  "site-packages/finetuning_scheduler/setup_tools.py",
  "site-packages/fts_examples/__init__.py",
  "site-packages/fts_examples/config/RteBoolqModule_ft_schedule_deberta_base.yaml",
  "site-packages/fts_examples/config/fts_defaults.yaml",
  "site-packages/fts_examples/config/fts_explicit.yaml",
  "site-packages/fts_examples/config/fts_implicit.yaml",
  "site-packages/fts_examples/config/nofts_baseline.yaml",
  "site-packages/fts_examples/fts_superglue.py",
  "site-packages/fts_examples/run_examples.sh",
  "site-packages/fts_examples/test_examples.py"
 ],
 "index": {
  "arch": null,
  "build": "pyhd8ed1ab_0",
  "build_number": 0,
  "depends": [
   "python >=3.7",
   "pytorch >=1.8",
   "pytorch-lightning >=1.6.0,<=1.6.3",
   "setuptools <59.6"
  ],
  "license": "Apache-2.0",
  "name": "finetuning-scheduler",
  "noarch": "python",
  "platform": null,
  "subdir": "noarch",
  "timestamp": 1651696208508,
  "version": "0.1.3"
 },
 "metadata_version": 1,
 "name": "finetuning-scheduler",
 "raw_recipe": "{% set name = \"finetuning-scheduler\" %}\n{% set version = \"0.1.3\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/finetuning-scheduler-{{ version }}.tar.gz\n  sha256: e58ab7c85d4f5ea73f94185b8d6647bdfebe51c71719a8f5515f00764bd7b66d\n\nbuild:\n  number: 0\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv\n\nrequirements:\n  host:\n    - pip\n    - python >=3.7\n  run:\n    - python >=3.7\n    - pytorch-lightning >=1.6.0, <=1.6.3\n    - setuptools <59.6\n    - pytorch >=1.8\n\ntest:\n  imports:\n    - finetuning_scheduler\n  requires:\n    - pip\n\nabout:\n  home: https://github.com/speediedan/finetuning-scheduler\n  summary: A PyTorch Lightning extension that enhances model experimentation with flexible finetuning schedules.\n  license: Apache-2.0\n  license_file: LICENSE\n  description: |\n    The FinetuningScheduler callback accelerates and enhances foundational model experimentation with flexible finetuning\n    schedules. Training with the FinetuningScheduler callback is simple and confers a host of benefits:\n\n    - it dramatically increases finetuning flexibility\n    - expedites and facilitates exploration of model tuning dynamics\n    - enables marginal performance improvements of finetuned models\n\n    Fundamentally, the FinetuningScheduler callback enables multi-phase, scheduled finetuning of foundational models.\n    Gradual unfreezing (i.e. thawing) can help maximize foundational model knowledge retention while allowing (typically\n    upper layers of) the model to optimally adapt to new tasks during transfer learning.\n\n    FinetuningScheduler orchestrates the gradual unfreezing of models via a finetuning schedule that is either implicitly\n    generated (the default) or explicitly provided by the user (more computationally efficient). Finetuning phase\n    transitions are driven by FTSEarlyStopping criteria (a multi-phase extension of EarlyStopping), user-specified epoch\n    transitions or a composition of the two (the default mode). A FinetuningScheduler training session completes when the\n    final phase of the schedule has its stopping criteria met.\n\n    Documentation\n    -------------\n    - https://finetuning-scheduler.readthedocs.io/en/stable/\n    - https://finetuning-scheduler.readthedocs.io/en/0.1.3/\n\nextra:\n  recipe-maintainers:\n    - speediedan\n",
 "rendered_recipe": {
  "about": {
   "description": "The FinetuningScheduler callback accelerates and enhances foundational model experimentation with flexible finetuning\nschedules. Training with the FinetuningScheduler callback is simple and confers a host of benefits:\n\n- it dramatically increases finetuning flexibility\n- expedites and facilitates exploration of model tuning dynamics\n- enables marginal performance improvements of finetuned models\n\nFundamentally, the FinetuningScheduler callback enables multi-phase, scheduled finetuning of foundational models.\nGradual unfreezing (i.e. thawing) can help maximize foundational model knowledge retention while allowing (typically\nupper layers of) the model to optimally adapt to new tasks during transfer learning.\n\nFinetuningScheduler orchestrates the gradual unfreezing of models via a finetuning schedule that is either implicitly\ngenerated (the default) or explicitly provided by the user (more computationally efficient). Finetuning phase\ntransitions are driven by FTSEarlyStopping criteria (a multi-phase extension of EarlyStopping), user-specified epoch\ntransitions or a composition of the two (the default mode). A FinetuningScheduler training session completes when the\nfinal phase of the schedule has its stopping criteria met.\n\nDocumentation\n-------------\n- https://finetuning-scheduler.readthedocs.io/en/stable/\n- https://finetuning-scheduler.readthedocs.io/en/0.1.3/\n",
   "home": "https://github.com/speediedan/finetuning-scheduler",
   "license": "Apache-2.0",
   "license_file": "LICENSE",
   "summary": "A PyTorch Lightning extension that enhances model experimentation with flexible finetuning schedules."
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "/home/conda/feedstock_root/build_artifacts/finetuning-scheduler_1651696126452/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_/bin/python -m pip install . -vv",
   "string": "pyhd8ed1ab_0"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "speediedan"
   ]
  },
  "package": {
   "name": "finetuning-scheduler",
   "version": "0.1.3"
  },
  "requirements": {
   "host": [
    "_libgcc_mutex 0.1 conda_forge",
    "_openmp_mutex 4.5 2_gnu",
    "bzip2 1.0.8 h7f98852_4",
    "ca-certificates 2021.10.8 ha878542_0",
    "ld_impl_linux-64 2.36.1 hea4e1c9_2",
    "libffi 3.4.2 h7f98852_5",
    "libgcc-ng 11.2.0 h1d223b6_16",
    "libgomp 11.2.0 h1d223b6_16",
    "libnsl 2.0.0 h7f98852_0",
    "libuuid 2.32.1 h7f98852_1000",
    "libzlib 1.2.11 h166bdaf_1014",
    "ncurses 6.3 h27087fc_1",
    "openssl 3.0.3 h166bdaf_0",
    "pip 22.0.4 pyhd8ed1ab_0",
    "python 3.10.4 h2660328_0_cpython",
    "python_abi 3.10 2_cp310",
    "readline 8.1 h46c0cb4_0",
    "setuptools 62.1.0 py310hff52083_0",
    "sqlite 3.38.3 h4ff8645_0",
    "tk 8.6.12 h27826a3_0",
    "tzdata 2022a h191b570_0",
    "wheel 0.37.1 pyhd8ed1ab_0",
    "xz 5.2.5 h516909a_1",
    "zlib 1.2.11 h166bdaf_1014"
   ],
   "run": [
    "python >=3.7",
    "pytorch >=1.8",
    "pytorch-lightning >=1.6.0, <=1.6.3",
    "setuptools <59.6"
   ]
  },
  "source": {
   "sha256": "e58ab7c85d4f5ea73f94185b8d6647bdfebe51c71719a8f5515f00764bd7b66d",
   "url": "https://pypi.io/packages/source/f/finetuning-scheduler/finetuning-scheduler-0.1.3.tar.gz"
  },
  "test": {
   "imports": [
    "finetuning_scheduler"
   ],
   "requires": [
    "pip"
   ]
  }
 },
 "version": "0.1.3"
}