{
 "about": {
  "channels": [
   "https://conda.anaconda.org/conda-forge"
  ],
  "conda_build_version": "3.22.0",
  "conda_private": false,
  "conda_version": "22.9.0",
  "dev_url": "https://github.com/huggingface/tokenizers",
  "env_vars": {
   "CIO_TEST": "<not set>"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari"
   ]
  },
  "home": "https://pypi.org/project/tokenizers/",
  "identifiers": [],
  "keywords": [],
  "license": "Apache-2.0",
  "license_family": "APACHE",
  "license_file": "LICENSE",
  "root_pkgs": [
   "pkginfo 1.8.3 pyhd8ed1ab_0",
   "tqdm 4.64.1 pyhd8ed1ab_0",
   "urllib3 1.26.11 pyhd8ed1ab_0",
   "libedit 3.1.20191231 he28a2e2_2",
   "backports 1.0 py_2",
   "libzlib 1.2.13 h166bdaf_4",
   "icu 70.1 h27087fc_0",
   "_openmp_mutex 4.5 2_gnu",
   "libmambapy 0.27.0 py310hab0e683_0",
   "pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0",
   "conda-package-handling 1.9.0 py310h5764c6d_0",
   "yaml-cpp 0.7.0 h27087fc_2",
   "libssh2 1.10.0 haa6b8db_3",
   "pyyaml 6.0 py310h5764c6d_4",
   "attrs 22.1.0 pyh71513ae_1",
   "backports.functools_lru_cache 1.6.4 pyhd8ed1ab_0",
   "bzip2 1.0.8 h7f98852_4",
   "requests 2.28.1 pyhd8ed1ab_1",
   "libsqlite 3.39.4 h753d276_0",
   "certifi 2022.9.24 pyhd8ed1ab_0",
   "watchgod 0.8.2 pyhd8ed1ab_0",
   "wheel 0.37.1 pyhd8ed1ab_0",
   "sniffio 1.3.0 pyhd8ed1ab_0",
   "python-dateutil 2.8.2 pyhd8ed1ab_0",
   "pycosat 0.6.3 py310h5764c6d_1010",
   "conda-build 3.22.0 py310hff52083_2",
   "setuptools 65.5.0 pyhd8ed1ab_0",
   "patchelf 0.15.0 h58526e2_0",
   "pcre2 10.37 hc3806b6_1",
   "patch 2.7.6 h7f98852_1002",
   "ca-certificates 2022.9.24 ha878542_0",
   "markupsafe 2.1.1 py310h5764c6d_1",
   "jupyter_core 4.11.1 py310hff52083_0",
   "colorama 0.4.5 pyhd8ed1ab_0",
   "libmamba 0.27.0 h0dd8ff0_0",
   "toolz 0.12.0 pyhd8ed1ab_0",
   "ripgrep 13.0.0 h2f28480_2",
   "pytz 2022.5 pyhd8ed1ab_0",
   "libcurl 7.85.0 h7bff187_0",
   "charset-normalizer 2.1.1 pyhd8ed1ab_0",
   "xz 5.2.6 h166bdaf_0",
   "keyutils 1.6.1 h166bdaf_0",
   "json5 0.9.5 pyh9f0ad1d_0",
   "libnsl 2.0.0 h7f98852_0",
   "krb5 1.19.3 h3790be6_0",
   "pycparser 2.21 pyhd8ed1ab_0",
   "lz4-c 1.9.3 h9c3ff4c_1",
   "libev 4.33 h516909a_1",
   "commonmark 0.9.1 py_0",
   "readline 8.1.2 h0f457ee_0",
   "git 2.38.1 pl5321h5fbbf19_0",
   "libgcc-ng 12.2.0 h65d4601_18",
   "jsonschema 4.16.0 pyhd8ed1ab_0",
   "mamba 0.27.0 py310hf87f941_0",
   "chardet 5.0.0 py310hff52083_0",
   "conda 22.9.0 py310hff52083_1",
   "python 3.10.6 h582c2e5_0_cpython",
   "pybind11-abi 4 hd8ed1ab_3",
   "importlib_resources 5.10.0 pyhd8ed1ab_0",
   "expat 2.4.9 h27087fc_0",
   "libnghttp2 1.47.0 hdcd2b5c_1",
   "wcwidth 0.2.5 pyh9f0ad1d_2",
   "soupsieve 2.3.2.post1 pyhd8ed1ab_0",
   "importlib-metadata 4.11.4 py310hff52083_0",
   "glob2 0.7 py_0",
   "psutil 5.9.3 py310h5764c6d_0",
   "python-fastjsonschema 2.16.2 pyhd8ed1ab_0",
   "libarchive 3.5.2 hb890918_3",
   "cffi 1.15.1 py310h255011f_1",
   "joblib 1.2.0 pyhd8ed1ab_0",
   "ncurses 6.3 h27087fc_1",
   "pysocks 1.7.1 pyha2e5f31_6",
   "cryptography 38.0.2 py310h597c629_0",
   "libiconv 1.17 h166bdaf_0",
   "rich 12.6.0 pyhd8ed1ab_0",
   "libgomp 12.2.0 h65d4601_18",
   "libstdcxx-ng 12.2.0 h46fd767_18",
   "tk 8.6.12 h27826a3_0",
   "libuuid 2.32.1 h7f98852_1000",
   "libxml2 2.9.14 h22db469_4",
   "libffi 3.4.2 h7f98852_5",
   "zipp 3.9.0 pyhd8ed1ab_0",
   "c-ares 1.18.1 h7f98852_0",
   "su-exec 0.2 h166bdaf_1003",
   "reproc 14.2.3 h7f98852_0",
   "zstd 1.5.2 h6239696_4",
   "openssl 1.1.1q h166bdaf_1",
   "gettext 0.21.1 h27087fc_0",
   "ruamel.yaml.clib 0.2.6 py310h5764c6d_1",
   "py-lief 0.12.2 py310hd8f1fbe_0",
   "six 1.16.0 pyh6c4a22f_0",
   "idna 3.4 pyhd8ed1ab_0",
   "typing_extensions 4.4.0 pyha770c72_0",
   "beautifulsoup4 4.11.1 pyha770c72_0",
   "perl 5.32.1 2_h7f98852_perl5",
   "lzo 2.10 h516909a_1000",
   "ruamel.yaml 0.17.21 py310h5764c6d_1",
   "traitlets 5.5.0 pyhd8ed1ab_0",
   "pygments 2.13.0 pyhd8ed1ab_0",
   "ruamel_yaml 0.15.80 py310h5764c6d_1007",
   "anyio 3.6.2 pyhd8ed1ab_0",
   "python-libarchive-c 4.0 py310hff52083_1",
   "nbformat 5.7.0 pyhd8ed1ab_0",
   "ld_impl_linux-64 2.39 hc81fddc_0",
   "tzdata 2022e h191b570_0",
   "brotlipy 0.7.0 py310h5764c6d_1004",
   "pyopenssl 22.1.0 pyhd8ed1ab_0",
   "python_abi 3.10 2_cp310",
   "reproc-cpp 14.2.3 h9c3ff4c_0",
   "anaconda-client 1.8.0 pyhd8ed1ab_0",
   "jinja2 3.1.2 pyhd8ed1ab_1",
   "pyrsistent 0.18.1 py310h5764c6d_1",
   "future 0.18.2 py310hff52083_5",
   "filelock 3.8.0 pyhd8ed1ab_0",
   "_libgcc_mutex 0.1 conda_forge",
   "tini 0.19.0 h166bdaf_1",
   "clyent 1.2.2 py_1",
   "dataclasses 0.8 pyhc8e2a94_3",
   "toml 0.10.2 pyhd8ed1ab_0",
   "libsolv 0.7.22 h6239696_0",
   "prompt-toolkit 3.0.31 pyha770c72_0",
   "pip 22.3 pyhd8ed1ab_0",
   "yaml 0.2.5 h7f98852_2",
   "curl 7.85.0 h7bff187_0",
   "prompt_toolkit 3.0.31 hd8ed1ab_0",
   "boa 0.12.0 pyha770c72_4",
   "liblief 0.12.2 h27087fc_0",
   "conda-env 2.6.0 1",
   "oniguruma 6.9.8 h166bdaf_0",
   "jq 1.6 h36c2ea0_1000",
   "conda-forge-ci-setup 3.21.0 py310hce54274_100",
   "click 8.1.3 unix_pyhd8ed1ab_2",
   "shyaml 0.6.2 pyhd3deb0d_0"
  ],
  "summary": "Fast State-of-the-Art Tokenizers optimized for Research and Production",
  "tags": []
 },
 "conda_build_config": {
  "CI": "azure",
  "c_compiler": "gcc",
  "cdt_name": "cos6",
  "channel_sources": "conda-forge",
  "channel_targets": "conda-forge main",
  "cpu_optimization_target": "nocona",
  "cran_mirror": "https://cran.r-project.org",
  "cxx_compiler": "gxx",
  "cxx_compiler_version": "10",
  "docker_image": "quay.io/condaforge/linux-anvil-cos7-x86_64",
  "extend_keys": [
   "pin_run_as_build",
   "extend_keys",
   "ignore_version",
   "ignore_build_only_deps"
  ],
  "fortran_compiler": "gfortran",
  "ignore_build_only_deps": [
   "python",
   "numpy"
  ],
  "lua": "5",
  "numpy": "1.16",
  "openssl": "3",
  "perl": "5.26.2",
  "pin_run_as_build": {
   "python": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   },
   "r-base": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   }
  },
  "python": "3.11.* *_cpython",
  "r_base": "3.5",
  "rust_compiler": "rust",
  "target_platform": "linux-64"
 },
 "files": [
  "lib/python3.11/site-packages/tokenizers-0.13.1.dist-info/INSTALLER",
  "lib/python3.11/site-packages/tokenizers-0.13.1.dist-info/METADATA",
  "lib/python3.11/site-packages/tokenizers-0.13.1.dist-info/RECORD",
  "lib/python3.11/site-packages/tokenizers-0.13.1.dist-info/REQUESTED",
  "lib/python3.11/site-packages/tokenizers-0.13.1.dist-info/WHEEL",
  "lib/python3.11/site-packages/tokenizers-0.13.1.dist-info/direct_url.json",
  "lib/python3.11/site-packages/tokenizers/__init__.py",
  "lib/python3.11/site-packages/tokenizers/__init__.pyi",
  "lib/python3.11/site-packages/tokenizers/decoders/__init__.py",
  "lib/python3.11/site-packages/tokenizers/decoders/__init__.pyi",
  "lib/python3.11/site-packages/tokenizers/implementations/__init__.py",
  "lib/python3.11/site-packages/tokenizers/implementations/base_tokenizer.py",
  "lib/python3.11/site-packages/tokenizers/implementations/bert_wordpiece.py",
  "lib/python3.11/site-packages/tokenizers/implementations/byte_level_bpe.py",
  "lib/python3.11/site-packages/tokenizers/implementations/char_level_bpe.py",
  "lib/python3.11/site-packages/tokenizers/implementations/sentencepiece_bpe.py",
  "lib/python3.11/site-packages/tokenizers/implementations/sentencepiece_unigram.py",
  "lib/python3.11/site-packages/tokenizers/models/__init__.py",
  "lib/python3.11/site-packages/tokenizers/models/__init__.pyi",
  "lib/python3.11/site-packages/tokenizers/normalizers/__init__.py",
  "lib/python3.11/site-packages/tokenizers/normalizers/__init__.pyi",
  "lib/python3.11/site-packages/tokenizers/pre_tokenizers/__init__.py",
  "lib/python3.11/site-packages/tokenizers/pre_tokenizers/__init__.pyi",
  "lib/python3.11/site-packages/tokenizers/processors/__init__.py",
  "lib/python3.11/site-packages/tokenizers/processors/__init__.pyi",
  "lib/python3.11/site-packages/tokenizers/tokenizers.cpython-311-x86_64-linux-gnu.so",
  "lib/python3.11/site-packages/tokenizers/tools/__init__.py",
  "lib/python3.11/site-packages/tokenizers/tools/visualizer-styles.css",
  "lib/python3.11/site-packages/tokenizers/tools/visualizer.py",
  "lib/python3.11/site-packages/tokenizers/trainers/__init__.py",
  "lib/python3.11/site-packages/tokenizers/trainers/__init__.pyi"
 ],
 "index": {
  "arch": "x86_64",
  "build": "py311h0ecb24d_2",
  "build_number": 2,
  "depends": [
   "libgcc-ng >=12",
   "libstdcxx-ng >=12",
   "openssl >=3.0.5,<4.0a0",
   "python >=3.11,<3.12.0a0",
   "python_abi 3.11.* *_cp311"
  ],
  "license": "Apache-2.0",
  "license_family": "APACHE",
  "name": "tokenizers",
  "platform": "linux",
  "subdir": "linux-64",
  "timestamp": 1666939925845,
  "version": "0.13.1"
 },
 "metadata_version": 1,
 "name": "tokenizers",
 "raw_recipe": "{% set version = \"0.13.1\" %}\n\npackage:\n  name: tokenizers\n  version: {{ version }}\n\nsource:\n  url: https://github.com/huggingface/tokenizers/archive/refs/tags/python-v{{ version }}.tar.gz\n  sha256: 41cff8c8c87ba6dfbd9eb1d89b006aabb9c9823ffd09e281d6ddfb9ae695bd1a\n  patches:\n    - patches/0001-don-t-fork-on-windows.patch  # [win]\n\nbuild:\n  number: 2\n  missing_dso_whitelist:\n    - /usr/lib/libresolv.9.dylib  # [osx]\n    - /usr/lib64/libgcc_s.so.1  # [linux]\n  script:\n    {% if build_platform != target_platform %}\n    - export PYO3_CROSS_INCLUDE_DIR=$PREFIX/include\n    - export PYO3_CROSS_LIB_DIR=$SP_DIR/../\n    - export PYO3_CROSS_PYTHON_VERSION=$PY_VER\n    # see below for what OPENSSL_DIR should be pointing to:\n    # https://github.com/sfackler/rust-openssl/blob/openssl-sys-v0.9.72/openssl/src/lib.rs#L55-L56\n    - export OPENSSL_DIR=$PREFIX\n    {% endif %}\n    - cd bindings/python\n    - {{ PYTHON }} -m pip install . -vv\n\nrequirements:\n  build:\n    - python                                 # [build_platform != target_platform]\n    - cross-python_{{ target_platform }}     # [build_platform != target_platform]\n    - openssl                                # [build_platform != target_platform]\n    - {{ compiler('cxx') }}\n    - {{ compiler('rust') }}\n  host:\n    - python\n    - pip\n    - setuptools-rust >=0.11.5\n    - setuptools\n    - openssl    # [linux]\n  run:\n    - python\n\ntest:\n  imports:\n    - tokenizers\n    - tokenizers.models\n    - tokenizers.decoders\n    - tokenizers.normalizers\n    - tokenizers.pre_tokenizers\n    - tokenizers.processors\n    - tokenizers.trainers\n    - tokenizers.implementations\n  requires:\n    - pip\n    - pytest\n    # transitive dependency arrow-cpp stuck on openssl 1.1.1 for the moment\n    - datasets  # [openssl == \"1.1.1\"]\n    - numpy *\n    - requests\n    - curl *\n    # temp: fix until https://github.com/conda-forge/multiprocess-feedstock/pull/46\n    # percolates far enough so that the solver doesn't pull in an old version anymore\n    - dill >=0.3.6\n  source_files:\n    - bindings/python/tests\n  commands:\n    - pip check\n    # upstream requires running the tests from this directory\n    - cd bindings/python\n    # adapted from https://github.com/huggingface/tokenizers/blob/master/bindings/python/Makefile\n    - mkdir data\n    - curl https://norvig.com/big.txt > data/big.txt\n    {% set tests_to_skip = \"_not_a_real_test\" %}\n    # windows and expectation of forking -> not gonna happen\n    {% set tests_to_skip = tests_to_skip + \" or with_parallelism\" %}  # [win]\n    # cannot run tests for openssl 3 for the moment (missing dependency, see above)\n    - pytest -v tests -k \"not ({{ tests_to_skip }})\"  # [openssl == \"1.1.1\"]\n\nabout:\n  home: https://pypi.org/project/tokenizers/\n  license: Apache-2.0\n  license_family: APACHE\n  license_file: LICENSE\n  summary: Fast State-of-the-Art Tokenizers optimized for Research and Production\n  dev_url: https://github.com/huggingface/tokenizers\n\nextra:\n  recipe-maintainers:\n    - anthchirp\n    - ndmaxar\n    - oblute\n    - setu4993\n    - h-vetinari\n",
 "rendered_recipe": {
  "about": {
   "dev_url": "https://github.com/huggingface/tokenizers",
   "home": "https://pypi.org/project/tokenizers/",
   "license": "Apache-2.0",
   "license_family": "APACHE",
   "license_file": "LICENSE",
   "summary": "Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build": {
   "missing_dso_whitelist": [
    "/usr/lib64/libgcc_s.so.1"
   ],
   "number": "2",
   "script": [
    "cd bindings/python",
    "/home/conda/feedstock_root/build_artifacts/tokenizers_1666939402070/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_/bin/python -m pip install . -vv"
   ],
   "string": "py311h0ecb24d_2"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "anthchirp",
    "h-vetinari",
    "ndmaxar",
    "oblute",
    "setu4993"
   ]
  },
  "package": {
   "name": "tokenizers",
   "version": "0.13.1"
  },
  "requirements": {
   "build": [
    "_libgcc_mutex 0.1 conda_forge",
    "_openmp_mutex 4.5 2_gnu",
    "binutils_impl_linux-64 2.39 h6ceecb4_0",
    "binutils_linux-64 2.39 h5fc0e48_11",
    "gcc_impl_linux-64 10.4.0 h5231bdf_19",
    "gcc_linux-64 10.4.0 h9215b83_11",
    "gxx_impl_linux-64 10.4.0 h5231bdf_19",
    "gxx_linux-64 10.4.0 h6e491c6_11",
    "kernel-headers_linux-64 2.6.32 he073ed8_15",
    "ld_impl_linux-64 2.39 hc81fddc_0",
    "libgcc-devel_linux-64 10.4.0 hd38fd1e_19",
    "libgcc-ng 12.2.0 h65d4601_19",
    "libgomp 12.2.0 h65d4601_19",
    "libsanitizer 10.4.0 h5246dfb_19",
    "libstdcxx-devel_linux-64 10.4.0 hd38fd1e_19",
    "libstdcxx-ng 12.2.0 h46fd767_19",
    "libzlib 1.2.13 h166bdaf_4",
    "rust 1.64.0 h30be4a0_0",
    "rust-std-x86_64-unknown-linux-gnu 1.64.0 hc1431ca_0",
    "rust_linux-64 1.64.0 ha252368_1",
    "sysroot_linux-64 2.12 he073ed8_15"
   ],
   "host": [
    "_libgcc_mutex 0.1 conda_forge",
    "_openmp_mutex 4.5 2_gnu",
    "bzip2 1.0.8 h7f98852_4",
    "ca-certificates 2022.9.24 ha878542_0",
    "ld_impl_linux-64 2.39 hc81fddc_0",
    "libffi 3.4.2 h7f98852_5",
    "libgcc-ng 12.2.0 h65d4601_19",
    "libgomp 12.2.0 h65d4601_19",
    "libnsl 2.0.0 h7f98852_0",
    "libsqlite 3.39.4 h753d276_0",
    "libstdcxx-ng 12.2.0 h46fd767_19",
    "libuuid 2.32.1 h7f98852_1000",
    "libzlib 1.2.13 h166bdaf_4",
    "ncurses 6.3 h27087fc_1",
    "openssl 3.0.5 h166bdaf_2",
    "pip 22.3 pyhd8ed1ab_0",
    "python 3.11.0 ha86cf86_0_cpython",
    "readline 8.1.2 h0f457ee_0",
    "semantic_version 2.10.0 pyhd8ed1ab_0",
    "setuptools 65.5.0 pyhd8ed1ab_0",
    "setuptools-rust 1.5.2 pyhd8ed1ab_0",
    "tk 8.6.12 h27826a3_0",
    "typing_extensions 4.4.0 pyha770c72_0",
    "tzdata 2022e h191b570_0",
    "wheel 0.37.1 pyhd8ed1ab_0",
    "xz 5.2.6 h166bdaf_0"
   ],
   "run": [
    "libgcc-ng >=12",
    "libstdcxx-ng >=12",
    "openssl >=3.0.5,<4.0a0",
    "python >=3.11,<3.12.0a0",
    "python_abi 3.11.* *_cp311"
   ]
  },
  "source": {
   "patches": null,
   "sha256": "41cff8c8c87ba6dfbd9eb1d89b006aabb9c9823ffd09e281d6ddfb9ae695bd1a",
   "url": "https://github.com/huggingface/tokenizers/archive/refs/tags/python-v0.13.1.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt"
   ],
   "imports": [
    "tokenizers",
    "tokenizers.decoders",
    "tokenizers.implementations",
    "tokenizers.models",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers"
   ],
   "requires": [
    "curl *",
    "dill >=0.3.6",
    "numpy *",
    "pip",
    "pytest",
    "requests"
   ],
   "source_files": [
    "bindings/python/tests"
   ]
  }
 },
 "version": "0.13.1"
}