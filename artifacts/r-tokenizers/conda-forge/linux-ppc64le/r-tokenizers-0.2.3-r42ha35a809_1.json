{
 "about": {
  "channels": [
   "https://conda.anaconda.org/conda-forge"
  ],
  "conda_build_version": "3.23.3",
  "conda_version": "22.11.1",
  "env_vars": {
   "CIO_TEST": "<not set>"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "conda-forge/r"
   ]
  },
  "home": "https://lincolnmullen.com/software/tokenizers/",
  "identifiers": [],
  "keywords": [],
  "license": "MIT",
  "license_family": "MIT",
  "license_file": [
   "/home/conda/feedstock_root/build_artifacts/r-tokenizers_1671808584262/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol/lib/R/share/licenses/MIT",
   "LICENSE"
  ],
  "root_pkgs": [
   "reproc 14.2.3 h4e0d66e_0",
   "conda-package-handling 2.0.2 pyh38be061_0",
   "ruamel.yaml 0.17.21 py310h939259b_2",
   "pytz 2022.6 pyhd8ed1ab_0",
   "pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0",
   "_openmp_mutex 4.5 2_gnu",
   "toolz 0.12.0 pyhd8ed1ab_0",
   "xz 5.2.6 hb283c62_0",
   "anyio 3.6.2 pyhd8ed1ab_0",
   "conda-build 3.23.3 py310h194a6c8_0",
   "expat 2.5.0 hbbae597_0",
   "charset-normalizer 2.1.1 pyhd8ed1ab_0",
   "libgomp 12.2.0 hbc1322c_19",
   "liblief 0.12.3 hbbae597_0",
   "jinja2 3.1.2 pyhd8ed1ab_1",
   "pkginfo 1.9.2 pyhd8ed1ab_0",
   "libffi 3.4.2 h4e0d66e_5",
   "wheel 0.38.4 pyhd8ed1ab_0",
   "_libgcc_mutex 0.1 conda_forge",
   "pybind11-abi 4 hd8ed1ab_3",
   "beautifulsoup4 4.11.1 pyha770c72_0",
   "lzo 2.10 h6eb9509_1000",
   "ripgrep 13.0.0 h0cb23b2_2",
   "glob2 0.7 py_0",
   "libsolv 0.7.23 h26001cd_0",
   "ld_impl_linux-ppc64le 2.39 hea198f4_1",
   "libxml2 2.10.3 h3010393_0",
   "pyrsistent 0.19.2 py310h93ff066_0",
   "readline 8.1.2 h6828edc_0",
   "py-lief 0.12.3 py310h76aa780_0",
   "libzlib 1.2.13 hb283c62_4",
   "icu 70.1 h3b4ca64_0",
   "importlib-metadata 5.1.0 pyha770c72_0",
   "joblib 1.2.0 pyhd8ed1ab_0",
   "yaml-cpp 0.7.0 hbbae597_2",
   "libsqlite 3.39.4 hcc10993_0",
   "tzdata 2022g h191b570_0",
   "ncurses 6.3 hab78ccb_1",
   "bzip2 1.0.8 h4e0d66e_4",
   "jupyter_core 5.1.0 py310h194a6c8_0",
   "anaconda-client 1.11.0 pyhd8ed1ab_1",
   "pyyaml 6.0 py310h939259b_5",
   "ruamel_yaml 0.15.80 py310h939259b_1008",
   "prompt_toolkit 3.0.36 hd8ed1ab_0",
   "pcre2 10.40 h02375f6_0",
   "zipp 3.11.0 pyhd8ed1ab_0",
   "chardet 5.1.0 py310hd032262_0",
   "libedit 3.1.20191231 h41a240f_2",
   "perl 5.32.1 2_h4e0d66e_perl5",
   "pycosat 0.6.4 py310h939259b_1",
   "boa 0.14.0 pyhd8ed1ab_0",
   "markupsafe 2.1.1 py310h93ff066_2",
   "setuptools 65.5.1 pyhd8ed1ab_0",
   "pluggy 1.0.0 pyhd8ed1ab_5",
   "traitlets 5.7.1 pyhd8ed1ab_0",
   "fmt 9.1.0 h06f31f1_0",
   "libnsl 2.0.0 h4e0d66e_0",
   "future 0.18.2 pyhd8ed1ab_6",
   "dataclasses 0.8 pyhc8e2a94_3",
   "libuuid 2.32.1 h4e0d66e_1000",
   "jsonschema 4.17.3 pyhd8ed1ab_0",
   "tini 0.19.0 hb283c62_1",
   "libgcc-ng 12.2.0 hbc1322c_19",
   "prompt-toolkit 3.0.36 pyha770c72_0",
   "patch 2.7.6 h4e0d66e_1002",
   "requests 2.28.1 pyhd8ed1ab_1",
   "gettext 0.21.1 hbbae597_0",
   "su-exec 0.2 hb283c62_1003",
   "psutil 5.9.4 py310h939259b_0",
   "wcwidth 0.2.5 pyh9f0ad1d_2",
   "urllib3 1.26.13 pyhd8ed1ab_0",
   "brotlipy 0.7.0 py310h939259b_1005",
   "c-ares 1.18.1 h4e0d66e_0",
   "importlib_resources 5.10.1 pyhd8ed1ab_0",
   "libiconv 1.17 hb283c62_0",
   "zstd 1.5.2 h581a010_4",
   "python-libarchive-c 4.0 py310h194a6c8_2",
   "tk 8.6.12 h41c6715_0",
   "keyutils 1.6.1 hb283c62_0",
   "pip 22.3.1 pyhd8ed1ab_0",
   "attrs 22.1.0 pyh71513ae_1",
   "tqdm 4.64.1 pyhd8ed1ab_0",
   "cffi 1.15.1 py310h8b509f7_3",
   "conda 22.11.1 py310h194a6c8_1",
   "python-dateutil 2.8.2 pyhd8ed1ab_0",
   "watchgod 0.8.2 pyhd8ed1ab_0",
   "zstandard 0.19.0 py310hcc13191_1",
   "backports.functools_lru_cache 1.6.4 pyhd8ed1ab_0",
   "toml 0.10.2 pyhd8ed1ab_0",
   "python-fastjsonschema 2.16.2 pyhd8ed1ab_0",
   "commonmark 0.9.1 py_0",
   "platformdirs 2.6.0 pyhd8ed1ab_0",
   "pyopenssl 22.1.0 pyhd8ed1ab_0",
   "filelock 3.8.2 pyhd8ed1ab_0",
   "certifi 2022.12.7 pyhd8ed1ab_0",
   "nbformat 5.7.0 pyhd8ed1ab_0",
   "yaml 0.2.5 h4e0d66e_2",
   "sniffio 1.3.0 pyhd8ed1ab_0",
   "rich 12.6.0 pyhd8ed1ab_0",
   "json5 0.9.5 pyh9f0ad1d_0",
   "patchelf 0.17.0 hea85c5d_0",
   "six 1.16.0 pyh6c4a22f_0",
   "pysocks 1.7.1 pyha2e5f31_6",
   "ca-certificates 2022.12.7 h1084571_0",
   "python_abi 3.10 3_cp310",
   "libstdcxx-ng 12.2.0 h99369c6_19",
   "lz4-c 1.9.3 h3b9df90_1",
   "conda-package-streaming 0.7.0 pyhd8ed1ab_1",
   "idna 3.4 pyhd8ed1ab_0",
   "reproc-cpp 14.2.3 h3b9df90_0",
   "backports 1.0 pyhd8ed1ab_3",
   "pygments 2.13.0 pyhd8ed1ab_0",
   "colorama 0.4.6 pyhd8ed1ab_0",
   "ruamel.yaml.clib 0.2.7 py310h82c586f_1",
   "clyent 1.2.2 py_1",
   "libev 4.33 h6eb9509_1",
   "soupsieve 2.3.2.post1 pyhd8ed1ab_0",
   "typing_extensions 4.4.0 pyha770c72_0",
   "pycparser 2.21 pyhd8ed1ab_0",
   "krb5 1.20.1 h5977a33_0",
   "shyaml 0.6.2 pyhd3deb0d_0",
   "libssh2 1.10.0 hc6ee4b3_3",
   "conda-forge-ci-setup 3.24.7 py310he5b63f5_100",
   "libmamba 1.1.0 h676306b_3",
   "libcurl 7.87.0 hea9912c_0",
   "conda-env 2.6.0 1",
   "mamba 1.1.0 py310h400a96e_3",
   "cryptography 38.0.4 py310hba3a939_0",
   "openssl 3.0.7 h4194056_1",
   "libnghttp2 1.47.0 h7a83b9a_1",
   "jq 1.6 h339bb43_1000",
   "libmambapy 1.1.0 py310hd638ff3_3",
   "libarchive 3.6.2 h5a2b0a6_0",
   "python 3.10.8 h736889e_0_cpython",
   "oniguruma 6.9.8 hb283c62_0",
   "click 8.1.3 unix_pyhd8ed1ab_2",
   "curl 7.87.0 hea9912c_0",
   "git 2.39.0 pl5321h8969c65_0"
  ],
  "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. ",
  "tags": []
 },
 "conda_build_config": {
  "CI": "travis",
  "c_compiler": "gcc",
  "c_compiler_version": "10",
  "cdt_name": "cos7",
  "channel_sources": "conda-forge",
  "channel_targets": "conda-forge main",
  "cpu_optimization_target": "nocona",
  "cran_mirror": "https://cran.r-project.org",
  "cxx_compiler": "gxx",
  "cxx_compiler_version": "10",
  "docker_image": "quay.io/condaforge/linux-anvil-ppc64le",
  "extend_keys": [
   "extend_keys",
   "ignore_build_only_deps",
   "pin_run_as_build",
   "ignore_version"
  ],
  "fortran_compiler": "gfortran",
  "ignore_build_only_deps": [
   "python",
   "numpy"
  ],
  "lua": "5",
  "numpy": "1.16",
  "perl": "5.26.2",
  "pin_run_as_build": {
   "python": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   },
   "r-base": {
    "max_pin": "x.x",
    "min_pin": "x.x"
   }
  },
  "python": "3.10",
  "r_base": "4.2",
  "target_platform": "linux-ppc64le",
  "zip_keys": [
   [
    "c_compiler_version",
    "cxx_compiler_version"
   ]
  ]
 },
 "conda_pkg_format": "2",
 "files": [
  "lib/R/library/tokenizers/CITATION",
  "lib/R/library/tokenizers/DESCRIPTION",
  "lib/R/library/tokenizers/INDEX",
  "lib/R/library/tokenizers/LICENSE",
  "lib/R/library/tokenizers/Meta/Rd.rds",
  "lib/R/library/tokenizers/Meta/data.rds",
  "lib/R/library/tokenizers/Meta/features.rds",
  "lib/R/library/tokenizers/Meta/hsearch.rds",
  "lib/R/library/tokenizers/Meta/links.rds",
  "lib/R/library/tokenizers/Meta/nsInfo.rds",
  "lib/R/library/tokenizers/Meta/package.rds",
  "lib/R/library/tokenizers/Meta/vignette.rds",
  "lib/R/library/tokenizers/NAMESPACE",
  "lib/R/library/tokenizers/NEWS.md",
  "lib/R/library/tokenizers/R/tokenizers",
  "lib/R/library/tokenizers/R/tokenizers.rdb",
  "lib/R/library/tokenizers/R/tokenizers.rdx",
  "lib/R/library/tokenizers/data/Rdata.rdb",
  "lib/R/library/tokenizers/data/Rdata.rds",
  "lib/R/library/tokenizers/data/Rdata.rdx",
  "lib/R/library/tokenizers/doc/index.html",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.R",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.Rmd",
  "lib/R/library/tokenizers/doc/introduction-to-tokenizers.html",
  "lib/R/library/tokenizers/doc/tif-and-tokenizers.R",
  "lib/R/library/tokenizers/doc/tif-and-tokenizers.Rmd",
  "lib/R/library/tokenizers/doc/tif-and-tokenizers.html",
  "lib/R/library/tokenizers/help/AnIndex",
  "lib/R/library/tokenizers/help/aliases.rds",
  "lib/R/library/tokenizers/help/paths.rds",
  "lib/R/library/tokenizers/help/tokenizers.rdb",
  "lib/R/library/tokenizers/help/tokenizers.rdx",
  "lib/R/library/tokenizers/html/00Index.html",
  "lib/R/library/tokenizers/html/R.css",
  "lib/R/library/tokenizers/libs/tokenizers.so"
 ],
 "index": {
  "arch": "ppc64le",
  "build": "r42ha35a809_1",
  "build_number": 1,
  "depends": [
   "libgcc-ng >=12",
   "libstdcxx-ng >=12",
   "r-base >=4.2,<4.3.0a0",
   "r-rcpp >=0.12.3",
   "r-snowballc >=0.5.1",
   "r-stringi >=1.0.1"
  ],
  "license": "MIT",
  "license_family": "MIT",
  "name": "r-tokenizers",
  "platform": "linux",
  "subdir": "linux-ppc64le",
  "timestamp": 1671808873425,
  "version": "0.2.3"
 },
 "metadata_version": 1,
 "name": "r-tokenizers",
 "raw_recipe": "{% set version = \"0.2.3\" %}\n{% set posix = 'm2-' if win else '' %}\n{% set native = 'm2w64-' if win else '' %}\n\npackage:\n  name: r-tokenizers\n  version: {{ version|replace(\"-\", \"_\") }}\n\nsource:\n  fn: tokenizers_{{ version }}.tar.gz\n  url:\n    - {{ cran_mirror }}/src/contrib/tokenizers_{{ version }}.tar.gz\n    - {{ cran_mirror }}/src/contrib/Archive/tokenizers/tokenizers_{{ version }}.tar.gz\n  sha256: 626d6b48b79dc4c3c130aebe201aac620f93665e0c5a890c3b6ca25c465f4207\n\nbuild:\n  merge_build_host: true  # [win]\n  number: 1\n  skip: true  # [win32]\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\nrequirements:\n  build:\n    - {{ compiler('c') }}        # [not win]\n    - {{ compiler('cxx') }}      # [not win]\n    - {{ compiler('m2w64_c') }}        # [win]\n    - {{ compiler('m2w64_cxx') }}        # [win]\n    - {{ posix }}filesystem        # [win]\n    - {{ posix }}make\n    - {{ posix }}sed               # [win]\n    - {{ posix }}coreutils         # [win]\n    - {{ posix }}zip               # [win]\n  host:\n    - r-base\n    - r-rcpp >=0.12.3\n    - r-snowballc >=0.5.1\n    - r-stringi >=1.0.1\n  run:\n    - r-base\n    - {{ native }}gcc-libs         # [win]\n    - r-rcpp >=0.12.3\n    - r-snowballc >=0.5.1\n    - r-stringi >=1.0.1\n\ntest:\n  commands:\n    - $R -e \"library('tokenizers')\"           # [not win]\n    - \"\\\"%R%\\\" -e \\\"library('tokenizers')\\\"\"  # [win]\n\nabout:\n  home: https://lincolnmullen.com/software/tokenizers/\n  license: MIT\n  summary: \"Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for\\\n    \\ splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. \"\n  license_family: MIT\n\n  license_file:\n    - {{ environ[\"PREFIX\"] }}/lib/R/share/licenses/MIT\n    - LICENSE\nextra:\n  recipe-maintainers:\n    - conda-forge/r\n",
 "rendered_recipe": {
  "about": {
   "home": "https://lincolnmullen.com/software/tokenizers/",
   "license": "MIT",
   "license_family": "MIT",
   "license_file": [
    "/home/conda/feedstock_root/build_artifacts/r-tokenizers_1671808584262/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol/lib/R/share/licenses/MIT",
    "LICENSE"
   ],
   "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. "
  },
  "build": {
   "number": "1",
   "rpaths": [
    "lib/",
    "lib/R/lib/"
   ],
   "string": "r42ha35a809_1"
  },
  "extra": {
   "copy_test_source_files": true,
   "final": true,
   "recipe-maintainers": [
    "conda-forge/r"
   ]
  },
  "package": {
   "name": "r-tokenizers",
   "version": "0.2.3"
  },
  "requirements": {
   "build": [
    "_libgcc_mutex 0.1 conda_forge",
    "_openmp_mutex 4.5 2_gnu",
    "binutils_impl_linux-ppc64le 2.39 heb37b50_1",
    "binutils_linux-ppc64le 2.39 h5e55cfe_11",
    "gcc_impl_linux-ppc64le 10.4.0 hb2bed53_19",
    "gcc_linux-ppc64le 10.4.0 hc879d7e_11",
    "gxx_impl_linux-ppc64le 10.4.0 hb2bed53_19",
    "gxx_linux-ppc64le 10.4.0 h1410db0_11",
    "kernel-headers_linux-ppc64le 3.10.0 h23d7e6c_13",
    "ld_impl_linux-ppc64le 2.39 hea198f4_1",
    "libgcc-devel_linux-ppc64le 10.4.0 ha78d8ae_19",
    "libgcc-ng 12.2.0 hbc1322c_19",
    "libgomp 12.2.0 hbc1322c_19",
    "libsanitizer 10.4.0 h8902bdb_19",
    "libstdcxx-devel_linux-ppc64le 10.4.0 ha78d8ae_19",
    "libstdcxx-ng 12.2.0 h99369c6_19",
    "make 4.3 hf817498_1",
    "sysroot_linux-ppc64le 2.17 h395ec9b_13"
   ],
   "host": [
    "_libgcc_mutex 0.1 conda_forge",
    "_openmp_mutex 4.5 2_gnu",
    "_r-mutex 1.0.1 anacondar_1",
    "binutils_impl_linux-ppc64le 2.39 heb37b50_1",
    "bwidget 1.9.14 ha3edaa6_1",
    "bzip2 1.0.8 h4e0d66e_4",
    "c-ares 1.18.1 h4e0d66e_0",
    "ca-certificates 2022.12.7 h1084571_0",
    "cairo 1.16.0 h9ac834e_1014",
    "curl 7.87.0 hea9912c_0",
    "expat 2.5.0 hbbae597_0",
    "font-ttf-dejavu-sans-mono 2.37 hab24e00_0",
    "font-ttf-inconsolata 3.000 h77eed37_0",
    "font-ttf-source-code-pro 2.038 h77eed37_0",
    "font-ttf-ubuntu 0.83 hab24e00_0",
    "fontconfig 2.14.1 hac4b4ab_0",
    "fonts-conda-ecosystem 1 0",
    "fonts-conda-forge 1 0",
    "freetype 2.12.1 h90753b0_1",
    "fribidi 1.0.10 h339bb43_0",
    "gcc_impl_linux-ppc64le 12.2.0 h3a350b8_19",
    "gettext 0.21.1 hbbae597_0",
    "gfortran_impl_linux-ppc64le 12.2.0 h55702a3_19",
    "graphite2 1.3.13 hea85c5d_1001",
    "gsl 2.7 h68b80c3_0",
    "gxx_impl_linux-ppc64le 12.2.0 h3a350b8_19",
    "harfbuzz 6.0.0 h26ad9ac_0",
    "icu 70.1 h3b4ca64_0",
    "jpeg 9e hb283c62_2",
    "kernel-headers_linux-ppc64le 3.10.0 h23d7e6c_13",
    "keyutils 1.6.1 hb283c62_0",
    "krb5 1.20.1 h5977a33_0",
    "ld_impl_linux-ppc64le 2.39 hea198f4_1",
    "lerc 4.0.0 hbbae597_0",
    "libblas 3.9.0 16_linuxppc64le_openblas",
    "libcblas 3.9.0 16_linuxppc64le_openblas",
    "libcurl 7.87.0 hea9912c_0",
    "libdeflate 1.14 hb283c62_0",
    "libedit 3.1.20191231 h41a240f_2",
    "libev 4.33 h6eb9509_1",
    "libffi 3.4.2 h4e0d66e_5",
    "libgcc-devel_linux-ppc64le 12.2.0 hccfcbdc_19",
    "libgcc-ng 12.2.0 hbc1322c_19",
    "libgfortran-ng 12.2.0 hfdc3801_19",
    "libgfortran5 12.2.0 hda65b67_19",
    "libglib 2.74.1 h2cdde7a_1",
    "libgomp 12.2.0 hbc1322c_19",
    "libiconv 1.17 hb283c62_0",
    "liblapack 3.9.0 16_linuxppc64le_openblas",
    "libnghttp2 1.47.0 h7a83b9a_1",
    "libopenblas 0.3.21 pthreads_h60f2977_3",
    "libpng 1.6.39 hcc10993_0",
    "libsanitizer 12.2.0 h99369c6_19",
    "libssh2 1.10.0 hc6ee4b3_3",
    "libstdcxx-devel_linux-ppc64le 12.2.0 hccfcbdc_19",
    "libstdcxx-ng 12.2.0 h99369c6_19",
    "libtiff 4.5.0 h7f9fe1d_0",
    "libuuid 2.32.1 h4e0d66e_1000",
    "libwebp-base 1.2.4 hb283c62_0",
    "libxcb 1.13 h4e0d66e_1004",
    "libxml2 2.10.3 h3010393_0",
    "libzlib 1.2.13 hb283c62_4",
    "make 4.3 hf817498_1",
    "ncurses 6.3 hab78ccb_1",
    "openssl 3.0.7 h4194056_1",
    "pango 1.50.12 hcb42c12_1",
    "pcre2 10.40 h02375f6_0",
    "pixman 0.40.0 h339bb43_0",
    "pthread-stubs 0.4 h339bb43_1001",
    "r-base 4.2.2 h28ec8b7_3",
    "r-rcpp 1.0.9 r42ha35a809_2",
    "r-snowballc 0.7.0 r42h497a943_2",
    "r-stringi 1.7.8 r42h875155f_1",
    "readline 8.1.2 h6828edc_0",
    "sed 4.8 ha07698b_0",
    "sysroot_linux-ppc64le 2.17 h395ec9b_13",
    "tk 8.6.12 h41c6715_0",
    "tktable 2.10 h339bb43_3",
    "xorg-kbproto 1.0.7 h4e0d66e_1002",
    "xorg-libice 1.0.10 h4e0d66e_0",
    "xorg-libsm 1.2.3 h743bd3d_1000",
    "xorg-libx11 1.7.2 h4e0d66e_0",
    "xorg-libxau 1.0.9 h4e0d66e_0",
    "xorg-libxdmcp 1.1.3 h4e0d66e_0",
    "xorg-libxext 1.3.4 h4e0d66e_1",
    "xorg-libxrender 0.9.10 h4e0d66e_1003",
    "xorg-libxt 1.2.1 h4e0d66e_2",
    "xorg-renderproto 0.11.1 h4e0d66e_1002",
    "xorg-xextproto 7.3.0 h4e0d66e_1002",
    "xorg-xproto 7.0.31 h4e0d66e_1007",
    "xz 5.2.6 hb283c62_0",
    "zlib 1.2.13 hb283c62_4",
    "zstd 1.5.2 h581a010_4"
   ],
   "run": [
    "libgcc-ng >=12",
    "libstdcxx-ng >=12",
    "r-base >=4.2,<4.3.0a0",
    "r-rcpp >=0.12.3",
    "r-snowballc >=0.5.1",
    "r-stringi >=1.0.1"
   ]
  },
  "source": {
   "fn": "tokenizers_0.2.3.tar.gz",
   "sha256": "626d6b48b79dc4c3c130aebe201aac620f93665e0c5a890c3b6ca25c465f4207",
   "url": [
    "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.2.3.tar.gz",
    "https://cran.r-project.org/src/contrib/tokenizers_0.2.3.tar.gz"
   ]
  },
  "test": {
   "commands": [
    "$R -e \"library('tokenizers')\""
   ]
  }
 },
 "version": "0.2.3"
}